{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use an RDD-based API from [pyspark.mllib](https://spark.apache.org/docs/2.1.1/mllib-collaborative-filtering.html) instead and compare the performance and runtime with those in [Spark_MovieLens.ipynb](Spark_MovieLens.ipynb). **This notebook has to run in Spark environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load \"ratings.csv\" using ``sc.textFile`` and then convert it to the form of (user, item, rating) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_rating = sc.textFile(\"ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = movie_rating.take(1)[0]\n",
    "rating_data = movie_rating.filter(lambda x: x!=header).map(lambda x: x.split(\",\")).map(lambda x: (x[0],x[1],x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', u'31', u'2.5'), (u'1', u'1029', u'3.0'), (u'1', u'1061', u'3.0')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check three rows\n",
    "rating_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split the data into training/validation/testing sets using a 60/20/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, validation, test = rating_data.randomSplit([6, 2, 2], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again use the same grid search to find the optimal hyperparameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to perform grid search and find the best ALS model\n",
    "# based on the validation RMSE\n",
    "\n",
    "def GridSearch(train, valid, num_iterations, reg_param, n_factors):\n",
    "    min_rmse = float('inf')\n",
    "    best_n = -1\n",
    "    best_reg = 0\n",
    "    best_model = None\n",
    "    for n in n_factors:\n",
    "        for reg in reg_param:\n",
    "            model = ALS.train(train, rank = n, iterations = num_iterations, lambda_ = reg, seed = 0)\n",
    "            predictions = model.predictAll(valid.map(lambda x: (x[0], x[1])))\n",
    "            predictions = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "            rate_and_preds = valid.map(lambda x: ((int(x[0]), int(x[1])), float(x[2]))).join(predictions)\n",
    "            rmse = math.sqrt(rate_and_preds.map(lambda x: (x[1][0] - x[1][1])**2).mean())\n",
    "            print '{} latent factors and regularization = {}: validation RMSE is {}'.format(n, reg, rmse)\n",
    "            if rmse < min_rmse:\n",
    "                min_rmse = rmse\n",
    "                best_n = n\n",
    "                best_reg = reg\n",
    "                best_model = model\n",
    "                \n",
    "    pred = best_model.predictAll(train.map(lambda x: (x[0], x[1])))\n",
    "    pred = pred.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "    rate_and_preds = train.map(lambda x: ((int(x[0]), int(x[1])), float(x[2]))).join(pred)\n",
    "    train_rmse = math.sqrt(rate_and_preds.map(lambda x: (x[1][0] - x[1][1])**2).mean())               \n",
    "    print '\\nThe best model has {} latent factors and regularization = {}:'.format(best_n, best_reg)\n",
    "    print 'traning RMSE is {}; validation RMSE is {}'.format(train_rmse, min_rmse)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 latent factors and regularization = 0.05: validation RMSE is 1.01785508876\n",
      "6 latent factors and regularization = 0.1: validation RMSE is 0.95260048447\n",
      "6 latent factors and regularization = 0.2: validation RMSE is 0.930242493613\n",
      "6 latent factors and regularization = 0.4: validation RMSE is 0.992676760763\n",
      "6 latent factors and regularization = 0.8: validation RMSE is 1.20267669575\n",
      "8 latent factors and regularization = 0.05: validation RMSE is 1.02799666376\n",
      "8 latent factors and regularization = 0.1: validation RMSE is 0.95051746407\n",
      "8 latent factors and regularization = 0.2: validation RMSE is 0.928105681593\n",
      "8 latent factors and regularization = 0.4: validation RMSE is 0.992166572865\n",
      "8 latent factors and regularization = 0.8: validation RMSE is 1.2026779671\n",
      "10 latent factors and regularization = 0.05: validation RMSE is 1.04031892715\n",
      "10 latent factors and regularization = 0.1: validation RMSE is 0.954383495189\n",
      "10 latent factors and regularization = 0.2: validation RMSE is 0.928274141014\n",
      "10 latent factors and regularization = 0.4: validation RMSE is 0.992036695198\n",
      "10 latent factors and regularization = 0.8: validation RMSE is 1.20267697626\n",
      "12 latent factors and regularization = 0.05: validation RMSE is 1.04807897642\n",
      "12 latent factors and regularization = 0.1: validation RMSE is 0.95345109993\n",
      "12 latent factors and regularization = 0.2: validation RMSE is 0.9282413668\n",
      "12 latent factors and regularization = 0.4: validation RMSE is 0.991987281552\n",
      "12 latent factors and regularization = 0.8: validation RMSE is 1.20267476615\n",
      "\n",
      "The best model has 8 latent factors and regularization = 0.2:\n",
      "traning RMSE is 0.689245109103; validation RMSE is 0.928105681593\n",
      "Total Runtime: 74.40 seconds\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "ranks = [6, 8, 10, 12]\n",
    "reg_params = [0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "start_time = time.time()\n",
    "final_model = GridSearch(train, validation, num_iterations, reg_params, ranks)\n",
    "print 'Total Runtime: {:.2f} seconds'.format(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The runtime is about 74 seconds, much faster than the dataframe-based approach. Again, the model with 8 latent factors and lambda = 0.2 yields the best result. Let's do a second grid search around these values with more iterations (15) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 latent factors and regularization = 0.1: validation RMSE is 0.953764517796\n",
      "7 latent factors and regularization = 0.2: validation RMSE is 0.930742215865\n",
      "7 latent factors and regularization = 0.3: validation RMSE is 0.953111214918\n",
      "8 latent factors and regularization = 0.1: validation RMSE is 0.948017777334\n",
      "8 latent factors and regularization = 0.2: validation RMSE is 0.928782688855\n",
      "8 latent factors and regularization = 0.3: validation RMSE is 0.952880120979\n",
      "9 latent factors and regularization = 0.1: validation RMSE is 0.952357433726\n",
      "9 latent factors and regularization = 0.2: validation RMSE is 0.929219565168\n",
      "9 latent factors and regularization = 0.3: validation RMSE is 0.952972698613\n",
      "\n",
      "The best model has 8 latent factors and regularization = 0.2:\n",
      "traning RMSE is 0.685661645411; validation RMSE is 0.928782688855\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 15\n",
    "ranks = [7, 8, 9]\n",
    "reg_params = [0.1, 0.2, 0.3]\n",
    "\n",
    "final_model = GridSearch(train, validation, num_iterations, reg_params, ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with 8 latent factors and lambda = 0.2 is still the best one. And finally, let's check the testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing RMSE is 0.921089888718\n"
     ]
    }
   ],
   "source": [
    "predictions = final_model.predictAll(test.map(lambda x: (x[0], x[1]))) \n",
    "predictions = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "rates_and_preds = test.map(lambda x: ((int(x[0]), int(x[1])), float(x[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda x: (x[1][0] - x[1][1])**2).mean())\n",
    "print 'The testing RMSE is ' + str(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
